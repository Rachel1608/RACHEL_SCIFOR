{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88dd6261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n"
     ]
    }
   ],
   "source": [
    "# Python string \n",
    "a = 'Data'\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01b663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# String Indexing\n",
    "a = \"Dataset\"\n",
    "print(a[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a609ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "od d\n",
      " da\n",
      "od\n"
     ]
    }
   ],
   "source": [
    "#String slicing\n",
    "a = 'Good day'\n",
    "print(a[2:6])\n",
    "print(a[-4:-1]) # negative slicing\n",
    "print(a[1:5:2]) # increment of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4a39a",
   "metadata": {},
   "source": [
    "### STRING METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c025d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awesome\n"
     ]
    }
   ],
   "source": [
    "# string in lower case\n",
    "a = 'AWESOME'\n",
    "print(a.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ba6a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANDROMEDA\n"
     ]
    }
   ],
   "source": [
    "#string in upper case\n",
    "a= 'andromeda'\n",
    "print(a.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdbd1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter\n"
     ]
    }
   ],
   "source": [
    "# Replace string with another string\n",
    "b = 'Jupiter'\n",
    "print(b.replace('i','y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b73751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abraca', 'abra']\n"
     ]
    }
   ],
   "source": [
    "# Split the string\n",
    "b = 'Abracadabra'\n",
    "print(b.split('d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46075c6",
   "metadata": {},
   "source": [
    "### STRING CONCATENATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d078eb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna sleeps\n"
     ]
    }
   ],
   "source": [
    "a = 'Anna'\n",
    "b = 'sleeps'\n",
    "print(a+\" \"+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c2664",
   "metadata": {},
   "source": [
    "### CREATING A TEXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb405ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1\n",
    "file = open('dataset1.txt',\"w+\") #w+ means write and read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5fa9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    file.write(\"Line Number is %d\\r\\n\" % (i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b7bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859a02c",
   "metadata": {},
   "source": [
    "### APPEND DATA TO A FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63ea2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('textdata.txt','a+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b44ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    file1.write(\"Appending Line Number %d\\r\\n\" % (i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "993dffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2851032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"dataset.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb14f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if file.mode=='r':\n",
    "    content= file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6afcf254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line Number is 1/w/nLine Number is 2/w/nLine Number is 3/w/nLine Number is 4/w/nLine Number is 5/w/nLine Number is 1/w/nLine Number is 2/w/nLine Number is 3/w/nLine Number is 4/w/nLine Number is 5/w/nLine Number is 1/r/nLine Number is 2/r/nLine Number is 3/r/nLine Number is 4/r/nLine Number is 5/r/nLine Number is 1\n",
      "\n",
      "Line Number is 2\n",
      "\n",
      "Line Number is 3\n",
      "\n",
      "Line Number is 4\n",
      "\n",
      "Line Number is 5\n",
      "\n",
      "Line Number is 1\n",
      "\n",
      "Line Number is 2\n",
      "\n",
      "Line Number is 3\n",
      "\n",
      "Line Number is 4\n",
      "\n",
      "Line Number is 5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a2747c",
   "metadata": {},
   "source": [
    "### TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "934759a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e839fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jupiter is also known to be failed star'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text lowercase\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "input_text = \"Jupiter is also known to be Failed Star\"\n",
    "lowercase_text(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "410bba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For removing the digits\n",
    "def remove_num(text):\n",
    "    result = re.sub(r'\\d+',\"\",text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf6a26ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He bought  apples, only  apples were in his bag'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = 'He bought 10 apples, only 7 apples were in his bag'\n",
    "remove_num(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbf0aa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: inflect in f:\\anaconda\\lib\\site-packages (7.3.1)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in f:\\anaconda\\lib\\site-packages (from inflect) (10.1.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in f:\\anaconda\\lib\\site-packages (from inflect) (4.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in f:\\anaconda\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c901780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ram bought twenty-five kilogram rice and ten kilogram dhal\n"
     ]
    }
   ],
   "source": [
    "import inflect\n",
    "#Initialize inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "#Converting number to text\n",
    "def convert_num(text):\n",
    "    #split strings into list of texts\n",
    "    temp_string = text.split()\n",
    "    \n",
    "    #Initialize empty list\n",
    "    new_str = []\n",
    "    for word in temp_string:\n",
    "        if word.isdigit():\n",
    "            new_str.append(p.number_to_words(word))\n",
    "        else:\n",
    "            new_str.append(word)\n",
    "    #Join the texts of new str to form a string\n",
    "    temp_str = \" \".join(new_str)\n",
    "    return temp_str\n",
    "\n",
    "input_str = 'Ram bought 25 kilogram rice and 10 kilogram dhal'\n",
    "print(convert_num(input_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe9fccb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hai Its so wonderful to meet you How are you doing\n"
     ]
    }
   ],
   "source": [
    "#Remove punctuation\n",
    "def rem_punct(text):\n",
    "    translator= str.maketrans(\"\",\"\",string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "input_str='Hai!!!, Its so wonderful to meet you!, How are you doing??'\n",
    "print(rem_punct(input_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85192443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "050cf3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing nltk library\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7272ef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sky', 'beautiful', 'sun', 'rises', 'east']\n"
     ]
    }
   ],
   "source": [
    "#Remove stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "     stop_words = set(stopwords.words(\"english\"))  # Import stopwords from nltk library\n",
    "     word_tokens = word_tokenize(text)  # Import word_tokenize from nltk library\n",
    "\n",
    "     # Filter out punctuation and stopwords\n",
    "     filtered_text = [word for word in word_tokens if word not in string.punctuation]\n",
    "     filtered_text = [word for word in filtered_text if word not in stop_words]\n",
    "\n",
    "     return filtered_text\n",
    "\n",
    "text = \"Sky is beautiful when the sun rises in the east\"\n",
    "filtered_text = remove_stopwords(text)\n",
    "print(filtered_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4924e9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['studi',\n",
       " 'of',\n",
       " 'bird',\n",
       " 'known',\n",
       " 'as',\n",
       " 'ornitholog',\n",
       " ',',\n",
       " 'studi',\n",
       " 'of',\n",
       " 'space',\n",
       " 'and',\n",
       " 'cosmo',\n",
       " 'is',\n",
       " 'astrophys']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming\n",
    "#importing nltk's porter stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#stem words in the list of tokenised words\n",
    "def stem_words(text):\n",
    "  word_tokens = word_tokenize(text)\n",
    "  stems = [stemmer.stem(word) for word in word_tokens]\n",
    "  return stems\n",
    "\n",
    "text = 'Study of Birds known as Ornithology, Study of Space and Cosmos is Astrophysics'\n",
    "stem_words(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6702bd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Study', 'of', 'Birds', 'know', 'as', 'Ornithology', ',', 'Study', 'of', 'Space', 'and', 'Cosmos', 'be', 'Astrophysics']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lemmatize_word(text):\n",
    "  nltk.download('wordnet')\n",
    "  \n",
    "  # Tokenize the text\n",
    "  word_tokens = word_tokenize(text)\n",
    "  \n",
    "  # Lemmatize each word\n",
    "  lemmas = [wordnet.WordNetLemmatizer().lemmatize(word, pos='v') for word in word_tokens]\n",
    "  \n",
    "  return lemmas\n",
    "\n",
    "# Example usage\n",
    "text = \"Study of Birds known as Ornithology, Study of Space and Cosmos is Astrophysics\"\n",
    "lemmatized_words = lemmatize_word(text)\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d714c7",
   "metadata": {},
   "source": [
    "### Parts of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "921106db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"Is'nt\", 'NNP'),\n",
       " ('this', 'DT'),\n",
       " ('place', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('beautiful', 'JJ'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing tokenize library\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# download averaged perceptron tagger\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# convert text into word tokens with their tags\n",
    "def pos_tagg(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return pos_tag(word_tokens)\n",
    "\n",
    "# Example usage\n",
    "pos_tagg(\"Is'nt this place is beautiful?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2013905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to C:\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "\n",
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d08d04cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Airplane/NNP\n",
      "  is/VBZ\n",
      "  (NP a/DT source/NN)\n",
      "  of/IN\n",
      "  (NP transportation/NN)\n",
      "  we/PRP\n",
      "  travel/VBP\n",
      "  through/IN\n",
      "  (NP sky/NN))\n",
      "(NP a/DT source/NN)\n",
      "(NP transportation/NN)\n",
      "(NP sky/NN)\n"
     ]
    }
   ],
   "source": [
    "# CHUNKING\n",
    "\n",
    "# importing libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "def chunking(text, grammar):\n",
    "    # label words with pos  \n",
    "  word_tokens = word_tokenize(text)\n",
    "    # create chunk parser using grammar\n",
    "  word_pos = pos_tag(word_tokens)\n",
    "\n",
    "  chunkParser = nltk.RegexpParser(grammar)\n",
    "\n",
    "  tree = chunkParser.parse(word_pos)\n",
    "\n",
    "  for subtree in tree.subtrees():\n",
    "    print(subtree)\n",
    "\n",
    "sentence = 'Airplane is a source of transportation we travel through sky'\n",
    "\n",
    "# Regular expression grammar for Noun Phrase\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "chunking(sentence, grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0020dc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Neptune/NNP)\n",
      "  and/CC\n",
      "  (GPE Uranus/NNP)\n",
      "  are/VBP\n",
      "  considered/VBN\n",
      "  to/TO\n",
      "  (ORGANIZATION Ice/NNP Giants/NNPS)\n",
      "  in/IN\n",
      "  our/PRP$\n",
      "  Solar/JJ\n",
      "  System/NN)\n"
     ]
    }
   ],
   "source": [
    "#Named Entity Recoginition\n",
    "#Import tokenization and chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def ner(text):           \n",
    "  word_tokens = word_tokenize(text)\n",
    "\n",
    "  #pos tagging of words\n",
    "  word_pos = pos_tag(word_tokens)\n",
    "\n",
    "  #tree of word entities\n",
    "  print(ne_chunk(word_pos))\n",
    "\n",
    "text = 'Neptune and Uranus are considered to Ice Giants in our Solar System'\n",
    "ner(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a1dd815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pond']\n"
     ]
    }
   ],
   "source": [
    "#Regular expressions(re)\n",
    "import re\n",
    "\n",
    "sent = \"Pond filled with fishes and tortoises\"\n",
    "r2 = re.findall(r\"^\\w+\", sent)\n",
    "\n",
    "print(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84ceebe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pond', 'filled', 'with', 'fishes', 'and', 'tortoises']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sent = \"Pond filled with fishes and tortoises\"\n",
    "r2 = re.findall(r\"\\w+\", sent)  \n",
    "\n",
    "print(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b4b5794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pond', 'filled', 'with', 'fishes', 'and', 'tortoises']\n"
     ]
    }
   ],
   "source": [
    "# Using split function\n",
    "import re\n",
    "\n",
    "print((re.split(r'\\s', \"Pond filled with fishes and tortoises\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35bb05e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('art', 'attack')\n",
      "('ant', 'act')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "lists = ['art attack', 'ant act', 'airplane flies']\n",
    "\n",
    "for a in lists:\n",
    "    q = re.match(\"(a\\w+)\\W(a\\w+)\", a)\n",
    "\n",
    "    if q:\n",
    "        print(q.groups())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3fb4e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're looking for 'riding' in 'Nancy reads book.'no match found!\n",
      "You're looking for 'book' in 'Nancy reads book.'Found match!\n"
     ]
    }
   ],
   "source": [
    "#re,search\n",
    "import re\n",
    "\n",
    "pattern = [\"riding\", \"book\"]\n",
    "text = \"Nancy reads book.\"\n",
    "\n",
    "for p in pattern:\n",
    "    print(\"You're looking for '%s' in '%s'\" % (p, text), end='')\n",
    "\n",
    "    if re.search(p, text):\n",
    "        print('Found match!')\n",
    "    else:\n",
    "        print(\"no match found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b73c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XYX@gmail.com\n",
      " lmn@gmail.com\n",
      " efg@gmail.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "mail = \"Abc@gmail.com, XYX@gmail.com, lmn@gmail.com, efg@gmail.com\"\n",
    "\n",
    "emails = re.findall(r' [\\w\\.]+@[\\w\\.]+', mail)\n",
    "\n",
    "for e in emails:\n",
    "    print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
